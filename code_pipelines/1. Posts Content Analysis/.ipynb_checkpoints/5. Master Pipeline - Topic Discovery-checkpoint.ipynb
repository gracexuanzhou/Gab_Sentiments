{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "5d2a1d8b-07a5-432a-8377-100c054d17f7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">Python interpreter will be restarted.\n",
       "Collecting nltk\n",
       "  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\n",
       "Collecting tqdm\n",
       "  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\n",
       "Collecting click\n",
       "  Using cached click-8.0.3-py3-none-any.whl (97 kB)\n",
       "Collecting regex&gt;=2021.8.3\n",
       "  Using cached regex-2021.10.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\n",
       "Requirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\n",
       "Installing collected packages: tqdm, regex, click, nltk\n",
       "Successfully installed click-8.0.3 nltk-3.6.5 regex-2021.10.23 tqdm-4.62.3\n",
       "WARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\n",
       "You should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-133b296d-1e7d-489c-a0c9-c1015bc457d2/bin/python -m pip install --upgrade pip&#39; command.\n",
       "Python interpreter will be restarted.\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">Python interpreter will be restarted.\nCollecting nltk\n  Using cached nltk-3.6.5-py3-none-any.whl (1.5 MB)\nCollecting tqdm\n  Using cached tqdm-4.62.3-py2.py3-none-any.whl (76 kB)\nCollecting click\n  Using cached click-8.0.3-py3-none-any.whl (97 kB)\nCollecting regex&gt;=2021.8.3\n  Using cached regex-2021.10.23-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (764 kB)\nRequirement already satisfied: joblib in /databricks/python3/lib/python3.8/site-packages (from nltk) (1.0.1)\nInstalling collected packages: tqdm, regex, click, nltk\nSuccessfully installed click-8.0.3 nltk-3.6.5 regex-2021.10.23 tqdm-4.62.3\nWARNING: You are using pip version 21.0.1; however, version 21.3.1 is available.\nYou should consider upgrading via the &#39;/local_disk0/.ephemeral_nfs/envs/pythonEnv-133b296d-1e7d-489c-a0c9-c1015bc457d2/bin/python -m pip install --upgrade pip&#39; command.\nPython interpreter will be restarted.\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "0439abe4-6c4f-42d8-9d64-98b8935aa872",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
       "[nltk_data]   Package stopwords is already up-to-date!\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">[nltk_data] Downloading package stopwords to /root/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "from pyspark.conf import SparkConf\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "import requests\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.ml.feature import CountVectorizer , IDF\n",
    "from pyspark.ml.linalg import Vector, Vectors, SparseVector\n",
    "from pyspark.ml.clustering import LDA\n",
    "\n",
    "# Getting stopwords lists\n",
    "StopWords = stopwords.words(\"english\")\n",
    "stopwords_list = requests.get(\"https://gist.githubusercontent.com/rg089/35e00abf8941d72d419224cfd5b5925d/raw/12d899b70156fd0041fa9778d657330b024b959c/stopwords.txt\").content\n",
    "stopwords_2 = set(stopwords_list.decode().splitlines()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "a847742a-93d7-4b35-9d03-10b30d868a10",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Defining UDFs\n",
    "\n",
    "# Text cleaning\n",
    "def clean_body(x):\n",
    "  punc='!\"#$%&\\()*+,-./:;<=>?@[\\\\]^_`{|}~'\n",
    "  cleaned = x.lower()\n",
    "  cleaned = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', cleaned, flags=re.MULTILINE)\n",
    "  cleaned = re.sub(r'[u|a]\\d+.*', '', cleaned, flags=re.MULTILINE) # remove special line break characters\n",
    "  for ch in punc:\n",
    "    cleaned = cleaned.replace(ch, '')\n",
    "  return cleaned\n",
    "\n",
    "# Mapping ID of terms to words of the vocabulary\n",
    "def map_termID_to_Word(termIndices):\n",
    "      words = []\n",
    "      for termID in termIndices:\n",
    "          words.append(vocabArray[termID])\n",
    "      return words\n",
    "\n",
    "# Stopwords tokens filtering\n",
    "def stop_words_filter(x):\n",
    "  return (~x.isin(StopWords)) & (~x.isin(stopwords_2)) & (x.isNotNull()) & (F.length(x) > 2)\n",
    "\n",
    "clean_body_udf = F.udf(clean_body , StringType())\n",
    "udf_map_termID_to_Word = F.udf(map_termID_to_Word , ArrayType(StringType()))\n",
    "max_index_udf = F.udf(lambda x: int(np.argmax(x)), IntegerType())\n",
    "max_value_udf = F.udf(lambda x: float(np.amax(x)), FloatType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "8d7b34aa-9b41-4cce-9dec-b103053cff9d",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\"></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\"></div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "periods = [\n",
    "  \"2016-11\", \"2016-12\", \"2017-01\", \"2017-02\", \"2017-03\", \"2017-04\", \"2017-05\", \"2017-06\", \"2017-07\", \"2017-08\",\n",
    "  \"2017-09\", \"2017-10\", \n",
    "  \"2017-11\", \n",
    "  \"2017-12\", \n",
    "  \"2018-01\", \"2018-02\", \"2018-03\", \"2018-04\", \"2018-05\"\n",
    "]\n",
    "\n",
    "# Based on perplexity analysis\n",
    "topics_per_period = [\n",
    "  70, 27, 130, 100, 100, 80, 105, 120, 90, 110,\n",
    "  120, 145, 150, \n",
    "  160, \n",
    "  180, 160, 190, 180, 25\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "76fd3751-ac30-4bbd-ace0-ba91b4b4da69",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .ansiout {\n",
       "    display: block;\n",
       "    unicode-bidi: embed;\n",
       "    white-space: pre-wrap;\n",
       "    word-wrap: break-word;\n",
       "    word-break: break-all;\n",
       "    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n",
       "    font-size: 13px;\n",
       "    color: #555;\n",
       "    margin-left: 4px;\n",
       "    line-height: 19px;\n",
       "  }\n",
       "</style>\n",
       "<div class=\"ansiout\">2018-01 180\n",
       "TF-IDF\n",
       "LDA\n",
       "COMPRISING DATA\n",
       "SAVING DATA\n",
       "2018-02 160\n",
       "TF-IDF\n",
       "LDA\n",
       "COMPRISING DATA\n",
       "SAVING DATA\n",
       "2018-03 190\n",
       "TF-IDF\n",
       "LDA\n",
       "COMPRISING DATA\n",
       "SAVING DATA\n",
       "2018-04 180\n",
       "TF-IDF\n",
       "LDA\n",
       "COMPRISING DATA\n",
       "SAVING DATA\n",
       "2018-05 25\n",
       "TF-IDF\n",
       "LDA\n",
       "COMPRISING DATA\n",
       "SAVING DATA\n",
       "</div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "arguments": {},
       "data": "<div class=\"ansiout\">2018-01 180\nTF-IDF\nLDA\nCOMPRISING DATA\nSAVING DATA\n2018-02 160\nTF-IDF\nLDA\nCOMPRISING DATA\nSAVING DATA\n2018-03 190\nTF-IDF\nLDA\nCOMPRISING DATA\nSAVING DATA\n2018-04 180\nTF-IDF\nLDA\nCOMPRISING DATA\nSAVING DATA\n2018-05 25\nTF-IDF\nLDA\nCOMPRISING DATA\nSAVING DATA\n</div>",
       "datasetInfos": [],
       "metadata": {},
       "removedWidgets": [],
       "type": "html"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# For each period\n",
    "for i in range (0,  len(periods)):\n",
    "    period = periods[i]\n",
    "    num_topics = topics_per_period[i]\n",
    "    print (period, num_topics)\n",
    "    \n",
    "    # Read the period data and clean text\n",
    "    data_chunk = spark.read.option(\"header\",\"true\").parquet(\"dbfs:/mnt/group12/sentiment/created_at_month=\" + period + \"/\")\n",
    "    data_chunk_tokens = data_chunk.withColumn('cleaned_body', clean_body_udf(F.col('body')))\n",
    "    data_chunk_tokens = data_chunk_tokens.withColumn('tokens', F.filter(F.split(F.col('cleaned_body'), ' '), stop_words_filter))\n",
    "    data_chunk_tokens = data_chunk_tokens.filter(F.size(F.col('tokens')) > 2)\n",
    "\n",
    "    print ('TF-IDF')\n",
    "    # Text vectorization\n",
    "    cv = CountVectorizer(inputCol=\"tokens\", outputCol=\"raw_features\", vocabSize=10000, minDF=1, maxDF=0.9)\n",
    "    cvmodel = cv.fit(data_chunk_tokens)\n",
    "    result_cv = cvmodel.transform(data_chunk_tokens)\n",
    "    # TF-IDF\n",
    "    idf = IDF(inputCol=\"raw_features\", outputCol=\"features\")\n",
    "    idfModel = idf.fit(result_cv)\n",
    "    result_tfidf = idfModel.transform(result_cv) \n",
    "\n",
    "    print ('LDA')\n",
    "    # Topic discovery using LDA\n",
    "    max_iterations = 100\n",
    "    lda = LDA(k=num_topics, optimizer=\"online\")\n",
    "    lda.setMaxIter(max_iterations)\n",
    "    lda_model = lda.fit(result_tfidf.select(\"id\", \"features\"))\n",
    "    lda_data = lda_model.transform(result_tfidf.select(\"id\", \"features\"))\n",
    "    topic_description = lda_model.describeTopics()\n",
    "    vocabArray = cvmodel.vocabulary\n",
    "\n",
    "    print ('COMPRISING DATA')\n",
    "    # TOPICS DESCRIPTION\n",
    "    topic_description_mapped = topic_description.withColumn(\"topic_desc\", udf_map_termID_to_Word(F.col('termIndices')))\n",
    "\n",
    "    # COMPRISING TOPIC DATA\n",
    "    lda_data_topic = lda_data.withColumn('topic_discovery_id', max_index_udf(F.col('topicDistribution')))\n",
    "    lda_data_topic = lda_data_topic.withColumn('message_topic_weight', max_value_udf(F.col('topicDistribution')))\n",
    "    lda_data_topic = lda_data_topic.select('id', 'topic_discovery_id', 'message_topic_weight').join(topic_description_mapped, topic_description_mapped.topic == lda_data_topic.topic_discovery_id, \"inner\").select('id', 'topic_discovery_id', 'topic_desc', 'message_topic_weight', F.array_join(F.slice(F.col('topic_desc'), 1, 5), ' | ').alias('topic_discovery_title'), F.concat(F.lit(period + '_'), F.col('topic_discovery_id')).alias('topic_discovery_unique_id'))\n",
    "\n",
    "    lda_comprised_result = lda_data_topic.join(data_chunk_tokens, ['id'])\n",
    "    lda_comprised_result = lda_comprised_result.withColumn('created_at_month', F.lit(period))\n",
    "\n",
    "    print ('SAVING DATA')\n",
    "    # SAVING DATA\n",
    "    write_mode = 'append'\n",
    "    if (i == 0): # On the first iteration we overwrite the directory\n",
    "        write_mode = 'overwrite'\n",
    "    lda_comprised_result.write.mode(write_mode).partitionBy('created_at_month').parquet(\"dbfs:/mnt/group12/topic_discovery/\")\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "inputWidgets": {},
     "nuid": "7e06a34a-3375-47a3-a9ec-a1683a3e4505",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Master Pipeline: Topic Discovery",
   "notebookOrigID": 1910166227477376,
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
